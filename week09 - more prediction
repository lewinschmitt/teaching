### Script prepared by Lewin Schmitt for IBEI's MLSS course (2023)
### The CHES data is from Polk, Jonathan, Jan Rovny, Ryan Bakker, Erica Edwards, Liesbet Hooghe, Seth Jolly, Jelle Koedam, Filip Kostelka, Gary Marks, Gijs Schumacher, Marco Steenbergen, Milada Anna Vachudova and Marko Zilovic. 2017. "Explaining the salience of anti-elitism and reducing political corruption for political parties in Europe with the 2014 Chapel Hill Expert Survey data," Research & Politics (January-March): 1-9.
### See https://www.chesdata.eu/ches-europe#2014-chapel-hill-expert-survey

if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, caret, class, neuralnet)

set.seed(123)

data_2014 <- read.csv('https://www.chesdata.eu/s/2014_CHES_dataset_means.csv')
data_2014 <- data_2014 %>%
  mutate(eu_stance = ifelse(eu_position < 5.5, 'anti','pro'))

## RUN UNTIL HERE BEFORE HEADING INTO THE BREAK

# PART I: IMPROVE OUR KNN
# let's see if we can improve our KNN predictions from last week
# 0.7142857 at k = 16

# get additional variables
knn_data_2014 <- data_2014 %>%
  select(eu_stance, urban_rural, immigrate_policy, lrgen, galtan) %>%
  na.omit()
knn_predictors_2014 <- knn_data_2014[,c('urban_rural','immigrate_policy', 'lrgen', 'galtan')]
knn_outcomes_2014 <- factor(knn_data_2014[,'eu_stance'])

knn_results_2014_cv <- tibble(k = matrix(c(1:(nrow(knn_data_2014)-1)),ncol=1),
                  accuracy = NA)

for(k in 1:(nrow(knn_data_2014)-1)){
  eu_prediction_cv.temp<-class::knn.cv(train = knn_predictors_2014,
                                 cl = knn_outcomes_2014, k = k)
  knn_results_2014_cv$accuracy[k] <-
    sum(eu_prediction_cv.temp == knn_outcomes_2014)/length(eu_prediction_cv.temp)
}
knn_results_2014_cv %>%
  ggplot(aes(x = k, y = accuracy)) +
  geom_line()
knn_results_2014_cv %>%
  filter(accuracy == max(accuracy))

# Slightly better, but the KNN method may reach some limits here,
# probably due to the underlying relationship of the data.

## PART 2: PREDICTION ON NEW DATA ----
## Still, let's see how good our best KNN does on predicting new data for 2019: 

data_2019 <- read.csv('https://www.chesdata.eu/s/CHES2019V3.csv')
# again, we need to convert our outcome (eu_stance) into a categorical variable
median(data_2019$eu_position)
data_2019 <- data_2019 %>%
  mutate(eu_stance = ifelse(eu_position < 5.4, 'anti','pro'))
knn_data_2019 <- data_2019 %>%
  select(eu_stance, urban_rural, immigrate_policy, environment, lrgen, galtan) %>%
  na.omit()
knn_predictors_2019 <- knn_data_2019[,c('urban_rural','immigrate_policy', 'lrgen', 'galtan')]
knn_outcomes_2019 <- factor(knn_data_2019[,'eu_stance'])

knn_predicted_2019 <- class::knn(train = knn_predictors_2014, # we use our 2014 predictors to train the model
                      test = knn_predictors_2019, # and test it on our 2019 predictors
                      cl = knn_outcomes_2014, # for the training, we need to provide the outcomes of the training data --> 2014
                      k = 7)

# now we contrast those predictions with the actual values for 2019:
caret::confusionMatrix(data = knn_predicted_2019, 
                       reference = factor(knn_outcomes_2019))

# For comparison, again our old model (with loocv)
knn_predicted_2014 <- class::knn.cv(train = knn_predictors_2014, 
                               cl = knn_outcomes_2014, 
                               k = 7)
caret::confusionMatrix(data = knn_predicted_2014, 
                       reference = factor(knn_outcomes_2014))

# Pretty much the same outcome. 
# Plausible, since party positions are rather sticky.



## PART 3: NEURAL NETWORK -----
# Can a Neural Network predict eu_stance better? 
# We use the neuralnet package, a lightweight implementation for R
# https://cran.r-project.org/web/packages/neuralnet/neuralnet.pdf

# Step 1: preparing the data (2014 and 2019)
nn_data_2014 <- data_2014 %>%
  select(eu_stance, urban_rural, immigrate_policy, environment, lrgen, galtan) %>%
  na.omit()
# for neural networks, it is often beneficial to normalize inputs.
# here, we do so using the scale() function.
nn_data_2014[2:6] <- scale(nn_data_2014[2:6])

nn_data_2019 <- data_2019 %>%
  select(eu_stance, urban_rural, immigrate_policy, environment, lrgen, galtan) %>%
  na.omit()
nn_data_2019[2:6] <- scale(nn_data_2019[2:6])


## Step 1: fitting the model on our 2014 data
# For the neuralnet() function, you need to specify how your data is related:
# formula = outcome ~ predictor1 + predictor2 + ...

nn_2014 <- neuralnet(formula = eu_stance ~ urban_rural + immigrate_policy + environment + lrgen + galtan,
                data = nn_data_2014, 
                hidden = c(4), # here we set the number of neurons per layer
                linear.output = FALSE # when working with categorical outcomes, we turn this off. Set to TRUE for regression
                )
plot(nn_2014)

# let's add another hidden layer to the net:
nn_2014 <- neuralnet(formula = eu_stance ~ urban_rural + immigrate_policy + environment + lrgen + galtan,
                data = nn_data_2014, 
                hidden = c(5, 4, 2), 
                linear.output = FALSE
                )
plot(nn_2014)

# let's see how well it did (ideally, we'd use CV!)
nn_predicts_2014 <- predict(nn_2014,
                       newdata = nn_data_2014 %>% select(-eu_stance))
# convert predictions to binary outcomes
nn_predicts_2014 <- ifelse(nn_predicts_2014[, 1] > 0.5, "anti", "pro")
# see how well the model does:
caret::confusionMatrix(factor(nn_predicts_2014), factor(nn_data_2014$eu_stance))

# Much better than our KNN! And on the new 2019 data?
nn_predicts_2019 <- predict(nn_2014,
                          newdata = nn_data_2019 %>% select(-eu_stance))
nn_predicts_2019 <- ifelse(nn_predicts_2019[, 1] > 0.5, "anti", "pro")
caret::confusionMatrix(factor(nn_predicts_2019), factor(nn_data_2019$eu_stance))

# At present, our neural net loses a lot of accuracy when presenting it the new data.
# The dynamic architecture may make it more sensitive/vulnerable than the KNN to small changes in the data.
# We probably overfitted and should have used CV in the training process.

## Bonus exercises: ----
# Can you improve the neural net by adding additional predictor variables to it?

## Voluntary take-home exercise ----
# Train a neural network on 2014 data using CV, and see whether that improves the out-of-sample performance for 2019 predictions.
